{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e1f78b-d95b-417b-ac10-341d4ec25b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abinaya S\\medical-text-summarization\\env\\Lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at fine_tuned_bart_small were not used when initializing BartForConditionalGeneration: ['model.decoder.layers.0.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.0.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.0.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.0.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.0.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.0.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.1.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.1.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.1.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.1.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.1.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.1.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.10.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.10.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.10.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.10.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.10.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.10.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.11.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.11.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.11.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.11.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.11.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.11.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.2.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.2.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.2.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.2.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.2.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.2.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.3.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.3.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.3.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.3.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.3.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.3.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.4.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.4.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.4.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.4.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.4.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.4.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.5.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.5.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.5.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.5.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.5.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.5.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.6.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.6.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.6.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.6.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.6.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.6.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.7.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.7.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.7.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.7.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.7.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.7.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.8.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.8.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.8.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.8.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.8.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.8.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.9.encoder_attn.q_proj.base_layer.bias', 'model.decoder.layers.9.encoder_attn.q_proj.base_layer.weight', 'model.decoder.layers.9.encoder_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.9.encoder_attn.q_proj.lora_B.default.weight', 'model.decoder.layers.9.self_attn.q_proj.base_layer.bias', 'model.decoder.layers.9.self_attn.q_proj.base_layer.weight', 'model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.0.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.0.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.1.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.1.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.10.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.10.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.11.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.11.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.2.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.2.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.3.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.3.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.4.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.4.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.5.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.5.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.6.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.6.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.7.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.7.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.8.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.8.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.encoder.layers.9.self_attn.q_proj.base_layer.bias', 'model.encoder.layers.9.self_attn.q_proj.base_layer.weight', 'model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at fine_tuned_bart_small and are newly initialized: ['model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.10.encoder_attn.q_proj.bias', 'model.decoder.layers.10.encoder_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.11.encoder_attn.q_proj.bias', 'model.decoder.layers.11.encoder_attn.q_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.6.encoder_attn.q_proj.bias', 'model.decoder.layers.6.encoder_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.7.encoder_attn.q_proj.bias', 'model.decoder.layers.7.encoder_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.8.encoder_attn.q_proj.bias', 'model.decoder.layers.8.encoder_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.9.encoder_attn.q_proj.bias', 'model.decoder.layers.9.encoder_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"fine_tuned_bart_small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_bart_small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c58aac9-76b1-42d7-ae64-75ec3a9fd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\") \n",
    "\n",
    " \n",
    "\n",
    "def summarize(text):\n",
    "    # Define generation configuration\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=150,  # Max length for the summary\n",
    "        min_length=40,   # Min length for the summary\n",
    "        num_beams=4,     # Beam search parameter\n",
    "        length_penalty=2.0,  # Length penalty\n",
    "        no_repeat_ngram_size=3,  # Prevent repetition of n-grams\n",
    "        do_sample=False,  # Disable sampling (deterministic generation)\n",
    "        early_stopping=True  # Stop early if the summary is generated\n",
    "    )\n",
    "\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "    # Generate summary with the custom generation config\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], generation_config=generation_config)\n",
    "\n",
    "    # Decode and return the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20e23a2c-9ac6-4057-ae35-1fc9617217e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The. The patient desired natural childbirth. The baby was born at 7 pounds 12 ounces. The mother pushed for 1 hour 20 minutes, delivered the head in the LOA position. The placenta delivered spontaneously intact with 3-vessel cord at 1315.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"The patient presented to Labor and Delivery at 1630 with complaint of spontaneous rupture of membranes at 3 a.m. with regular contractions. Her vaginal exam at 1650 was 3, 100, -2. The patient desired natural childbirth. Due to prolonged rupture of membranes, penicillin prophylaxis was started at 1830. At 0130, she was 6, 100, -1. At 0450, bag was ruptured with meconium-stained amniotic fluid, and she was 8, 100, -1. At 0720, she was 8, 100, 0. At 0950, after epidural anesthesia, she was 10, 100, +1. She progressed to complete, 100 and +1 station at 1150 after resting for 4 hours after epidural anesthesia. Fetal heart tones in the second stage of labor were overall reassuring. She began pushing with direction. She pushed for 1 hour 20 minutes, delivered the head in the LOA position. Bulb suction of mouth and nose at the perineum. Shoulders delivered easily. Progressed to normal spontaneous vaginal delivery of viable female at 1310, delivered onto mom's abdomen. The cord was clamped x2 and cut. Baby was handed to the neonatologist with meconium-stained amniotic fluid. Apgars were 8, 9 and 9. Negative meconium below the cord. The weight was 7 pounds 12 ounces. Placenta delivered spontaneously intact with 3-vessel cord at 1315. Fundus massaged to firm.\n",
    "Hemostasis achieved easily with an estimated blood loss of 350 mL after Pitocin 30 units IV piggyback given. A second-degree perineal laceration was noted and repaired with 2-0 chromic with local anesthesia. The mother and baby tolerated delivery well.\"\"\"\n",
    "summary = summarize(input_text)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c7790-edb7-4ad3-b829-9f2c6ac14cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
